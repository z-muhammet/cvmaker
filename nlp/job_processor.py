import asyncio
import json
import logging
import os
import sys
from typing import Dict, List, Optional
from datetime import datetime
from dotenv import load_dotenv

from .nlpApi import ExtractJobData
from .job_analyzer import jobAnalyzer
from dbprocess.db_manager import db

import asyncio
import logging
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.mongodb import MongoDBJobStore
from apscheduler.executors.pool import ThreadPoolExecutor
from dbprocess.db_manager import dbManager
import os
from dotenv import load_dotenv

load_dotenv()

MONGODB_URI = os.getenv("MONGODB_URI", "mongodb://localhost:27017")
MONGODB_DB = os.getenv("MONGODB_DB", "jobscrapper")

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

class jobProcessor:
  def __init__( self, batch_size: int = 10, max_retries: int = 3 ):
    self.batch_size = batch_size
    self.max_retries = max_retries
    self.processed_count = 0
    self.success_count = 0
    self.error_count = 0
    self.skipped_count = 0
    self.job_analyzer = None
    load_dotenv()

  async def InitializeJobAnalyzer( self ):
    try:
      self.job_analyzer = jobAnalyzer(use_browser = True)
      await self.job_analyzer.initialize_client()
      logger.info("âœ… jobAnalyzer baÅŸarÄ±yla baÅŸlatÄ±ldÄ±")
      return True
    except Exception as e:
      logger.error(f"âŒ jobAnalyzer baÅŸlatÄ±lamadÄ±: {e}")
      return False

  async def IsJobAlreadyAnalyzed( self, job_id ) -> bool:
    try:
      existing = await db.FindOne(
        "job_analysis_results",
        {"job_id": job_id}
      )
      return existing is not None
    except Exception as e:
      logger.error(f"âŒ Ã‡ift kontrol hatasÄ± (job {job_id}): {e}")
      return False

  def CombineJobText( self, job_data: Dict ) -> str:
    try:
      description = job_data.get('description', '')
      location = job_data.get('location', '')
      long_location = job_data.get('long_location', '')
      combined_location = location
      if( long_location and long_location != location ):
        combined_location = f"{location} - {long_location}" if location else long_location
      combined_text = description
      if ( combined_location ):
        combined_text = f"Location: {combined_location}\n\n{description}"
      logger.info(f"BirleÅŸtirilmiÅŸ metin uzunluÄŸu: {len(combined_text)} karakter")
      return combined_text
    except Exception as e:
      logger.error(f"Ä°ÅŸ metni birleÅŸtirme hatasÄ±: {e}")
      return job_data.get('description', '')

  async def ProcessJobWithNlpApi( self, job_text: str, job_id: str ) -> Optional[Dict]:
    for att in range(self.max_retries):
      try:
        logger.info(f"Job {job_id} NLP API ile iÅŸleniyor (deneme {att + 1}/{self.max_retries})")
        result = await ExtractJobData(job_text)
        if result and isinstance(result, dict):
          logger.info(f"âœ… Job {job_id} NLP API ile baÅŸarÄ±yla iÅŸlendi")
          return result
        else:
          logger.warning(f"âš ï¸ NLP API job {job_id} iÃ§in geÃ§ersiz sonuÃ§ dÃ¶ndÃ¼rdÃ¼")
      except Exception as e:
        logger.error(f"âŒ NLP API hatasÄ± job {job_id} (deneme {att + 1}): {e}")
        if att < self.max_retries - 1:
          await asyncio.sleep(2 ** att)
    return None

  async def ProcessJobWithPlaywright( self, job_text: str, job_id: str ) -> Optional[Dict]:
    try:
      if not self.job_analyzer:
        if not await self.InitializeJobAnalyzer():
          return None
      logger.info(f"Job {job_id} Playwright sistemi ile iÅŸleniyor")
      result = await self.job_analyzer.analyze_job_description(job_text)
      if result and not result.get('error'):
        logger.info(f"âœ… Job {job_id} Playwright ile baÅŸarÄ±yla iÅŸlendi")
        return result
      else:
        logger.warning(f"âš ï¸ Playwright job {job_id} iÃ§in hata dÃ¶ndÃ¼rdÃ¼: {result.get('error', 'Bilinmeyen hata')}")
        return None
    except Exception as e:
      logger.error(f"âŒ Playwright hatasÄ± job {job_id}: {e}")
      return None

  async def SaveAnalysisResult( self, job_id: str, analysis_result: Dict, original_job: Dict ):
    try:
      source_url = original_job.get('source_url', original_job.get('url', ''))
      if source_url:
        analysis_result['source_url'] = source_url
      analysis_doc = {
        "job_id": job_id,
        "analysis_result": analysis_result,
        "processed_at": datetime.now(),
        "processor_version": "1.0"
      }
      await db.InsertOne("job_analysis_results", analysis_doc)
      logger.info(f"âœ… Analiz sonucu kaydedildi job {job_id} (source_url: {source_url[:50] if source_url else 'YOK'}...)")
    except Exception as e:
      logger.error(f"âŒ Analiz sonucu kaydedilemedi job {job_id}: {e}")
      raise

  async def MarkJobAsProcessed( self, job_id ):
    try:
      job = await db.FindOne("raw_jobs", {"_id": job_id})
      if not job:
        logger.warning(f"âš ï¸ Job {job_id} raw_jobs koleksiyonunda bulunamadÄ±")
        return
      await db.UpdateOne(
        "raw_jobs",
        {"_id": job_id},
        {"$set": {"processed": True, "processed_at": datetime.now()}}
      )
      logger.info(f"âœ… Job {job_id} iÅŸlendi olarak iÅŸaretlendi")
    except Exception as e:
      logger.error(f"âŒ Job {job_id} iÅŸlendi olarak iÅŸaretlenemedi: {e}")

  async def GetUnprocessedJobs( self, limit: int = None ) -> List[Dict]:
    try:
      query = {"processed": {"$ne": True}}
      jobs = await db.FindMany("raw_jobs", query, limit or self.batch_size)
      if not jobs:
        logger.info("'processed' alanÄ± olan iÅŸ bulunamadÄ±, tÃ¼m iÅŸler alÄ±nÄ±yor...")
        jobs = await db.FindMany("raw_jobs", {}, limit or self.batch_size)
      logger.info(f"ğŸ“Š {len(jobs)} iÅŸ iÅŸlenmemiÅŸ olarak bulundu")
      return jobs
    except Exception as e:
      logger.error(f"âŒ Ä°ÅŸlenmemiÅŸ iÅŸler alÄ±namadÄ±: {e}")
      return []

  async def ProcessSingleJob( self, job_data: Dict ) -> bool:
    job_id = job_data.get('_id')
    try:
      logger.info(f"ğŸ”„ Job {job_id} iÅŸleniyor")
      if await self.IsJobAlreadyAnalyzed(job_id):
        logger.info(f"âš ï¸ Job {job_id} zaten analiz edilmiÅŸ, atlanÄ±yor")
        await self.MarkJobAsProcessed(job_id)
        self.skipped_count += 1
        return True
      job_text = self.CombineJobText(job_data)
      if not job_text or len(job_text.strip()) < 10:
        logger.warning(f"âš ï¸ Job {job_id} yeterli metne sahip deÄŸil, atlanÄ±yor")
        await self.MarkJobAsProcessed(job_id)
        self.skipped_count += 1
        return True
      analysis_result = await self.ProcessJobWithNlpApi(job_text, job_id)
      if not analysis_result:
        logger.info(f"ğŸ”„ NLP API baÅŸarÄ±sÄ±z oldu job {job_id}, Playwright deneniyor...")
        analysis_result = await self.ProcessJobWithPlaywright(job_text, job_id)
      if not analysis_result:
        logger.error(f"âŒ Hem NLP API hem Playwright baÅŸarÄ±sÄ±z oldu job {job_id}")
        self.error_count += 1
        return False
      await self.SaveAnalysisResult(job_id, analysis_result, job_data)
      await self.MarkJobAsProcessed(job_id)
      self.success_count += 1
      logger.info(f"âœ… Job {job_id} baÅŸarÄ±yla iÅŸlendi")
      return True
    except Exception as e:
      logger.error(f"âŒ Beklenmeyen hata job {job_id} iÅŸlenirken: {e}")
      self.error_count += 1
      return False

  async def ProcessAllJobs( self ):
    logger.info("ğŸš€ Ä°ÅŸ iÅŸleme baÅŸlatÄ±lÄ±yor...")
    total_processed = 0
    consecutive_empty_batches = 0
    max_empty_batches = 3
    while True:
      jobs = await self.GetUnprocessedJobs(self.batch_size)
      if not jobs:
        consecutive_empty_batches += 1
        logger.info(f"ğŸ“­ Ä°ÅŸlenmemiÅŸ iÅŸ bulunamadÄ± (boÅŸ batch {consecutive_empty_batches}/{max_empty_batches})")
        if consecutive_empty_batches >= max_empty_batches:
          logger.info("ğŸ“­ Maksimum boÅŸ batch sayÄ±sÄ±na ulaÅŸÄ±ldÄ±, duruluyor...")
          break
        await asyncio.sleep(5)
        continue
      consecutive_empty_batches = 0
      logger.info(f"ğŸ“¦ {len(jobs)} iÅŸten oluÅŸan batch iÅŸleniyor")
      for job in jobs:
        success = await self.ProcessSingleJob(job)
        total_processed += 1
        await asyncio.sleep(1)
      logger.info(f"ğŸ“Š Batch tamamlandÄ±. Toplam iÅŸlenen: {total_processed}")
      remaining_jobs = await self.GetUnprocessedJobs(1)
      if not remaining_jobs:
        logger.info("ğŸ“­ Ä°ÅŸlenecek baÅŸka iÅŸ kalmadÄ±")
        break
    logger.info("=" * 60)
    logger.info("ğŸ“Š FÄ°NAL Ä°ÅLEME Ä°STATÄ°STÄ°KLERÄ°")
    logger.info("=" * 60)
    logger.info(f"Toplam iÅŸlenen iÅŸ: {total_processed}")
    logger.info(f"BaÅŸarÄ±yla analiz edilen: {self.success_count}")
    logger.info(f"Atlanan (zaten analizli): {self.skipped_count}")
    logger.info(f"BaÅŸarÄ±sÄ±z: {self.error_count}")
    if total_processed > 0:
      success_rate = ((self.success_count + self.skipped_count) / total_processed * 100)
      logger.info(f"Genel baÅŸarÄ± oranÄ±: {success_rate:.2f}%")
    logger.info("=" * 60)

  async def Cleanup( self ):
    if self.job_analyzer:
      await self.job_analyzer.close()
      logger.info("ğŸ§¹ Temizlik tamamlandÄ±")

async def Main():
  import argparse
  parser = argparse.ArgumentParser(description = "VeritabanÄ±ndaki iÅŸleri NLP API ile iÅŸle")
  parser.add_argument('--batch-size', type = int, default = 10, help = 'Ä°ÅŸleme batch boyutu')
  parser.add_argument('--max-retries', type = int, default = 3, help = 'Her iÅŸ iÃ§in maksimum deneme')
  parser.add_argument('--test-single', type = str, help = 'Tek bir iÅŸ ID ile test')
  args = parser.parse_args()
  processor = jobProcessor(
    batch_size = args.batch_size,
    max_retries = args.max_retries
  )
  try:
    if args.test_single:
      job = await db.FindOne("raw_jobs", {"_id": args.test_single})
      if job:
        await processor.ProcessSingleJob(job)
      else:
        logger.error(f"Job {args.test_single} bulunamadÄ±")
    else:
      await processor.ProcessAllJobs()
  except KeyboardInterrupt:
    logger.info("â¹ï¸ Ä°ÅŸleme kullanÄ±cÄ± tarafÄ±ndan durduruldu")
  except Exception as e:
    logger.error(f"âŒ Beklenmeyen hata: {e}")
  finally:
    await processor.Cleanup()

if __name__ == "__main__":
  asyncio.run(Main()) 